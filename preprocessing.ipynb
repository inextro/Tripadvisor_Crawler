{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    \n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    \n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    \n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Define words in hotel name as stopwords.\n",
    "hotel_names = [hotel_name[:-4] for hotel_name in os.listdir('crawled_data')]\n",
    "cleaned_names = [str.lower(re.sub(pattern='[^a-zA-Z]', repl=' ', string=name)) for name in hotel_names]\n",
    "\n",
    "hotel_name_words = []\n",
    "\n",
    "for i in range(len(cleaned_names)):\n",
    "    temp = cleaned_names[i].split()\n",
    "    \n",
    "    for j in range(len(temp)):\n",
    "        hotel_name_words.append(temp[j])\n",
    "        \n",
    "hotel_name_words = [word for word in hotel_name_words]\n",
    "hotel_name_words = list(set(hotel_name_words)) # remove duplicated words\n",
    "\n",
    "hotel_name_stopwords = pd.DataFrame(data=hotel_name_words, columns=['stopwords'])\n",
    "hotel_name_stopwords.to_excel('preprocessing/stopwords_hotel.xlsx', index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotel_list = os.listdir('crawled_data')\n",
    "stopwords_hotel = pd.read_excel('preprocessing/stopwords_hotel.xlsx')\n",
    "stopwords_custom = pd.read_excel('preprocessing/stopwords_custom.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DT Matrix, TF-IDF Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, hotel in enumerate(hotel_list):\n",
    "    print(str(i + 1) + 'th hotel out of ' + str(len(hotel_list)) + ' hotels.')\n",
    "\n",
    "    data = pd.read_csv('crawled_data/' + hotel, sep='|', usecols=['review_id', 'review'])\n",
    "    \n",
    "    # tokenizing\n",
    "    data['review'] = data['review'].apply(lambda review: re.sub(pattern='[^a-zA-Z]', repl=' ', string=review)) # remove non-English character\n",
    "    data['review'] = data['review'].apply(str.lower) # to lower-case\n",
    "    data['review'] = data['review'].apply(word_tokenize)\n",
    "    \n",
    "    # pos tagging\n",
    "    documents = list(data['review'])\n",
    "    pos_documents = [pos_tag(document) for document in documents]\n",
    "    \n",
    "    # stopwords\n",
    "    stop_documents = []\n",
    "      \n",
    "    for pos_document in pos_documents:\n",
    "        stop_document = [tagged_word for tagged_word in pos_document if tagged_word[0] not in stopwords.words('english')]\n",
    "        stop_document = [tagged_word for tagged_word in stop_document if tagged_word[0] not in list(stopwords_hotel['stopwords'])]\n",
    "        stop_document = [tagged_word for tagged_word in stop_document if tagged_word[1].startswith(('J', 'V', 'N', 'R'))]\n",
    "        stop_documents.append(stop_document)\n",
    "        \n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_documents = []\n",
    "    \n",
    "    for stop_document in stop_documents:\n",
    "        lemmatized_document = [lemmatizer.lemmatize(tagged_word[0], pos=get_wordnet_pos(tagged_word[1])) for tagged_word in stop_document]\n",
    "        lemmatized_documents.append(lemmatized_document)\n",
    "    \n",
    "    # stopwords_custom\n",
    "    lemmatized_documents_ = []\n",
    "    \n",
    "    for lemmatized_document in lemmatized_documents:\n",
    "        lemmatized_document = [word for word in lemmatized_document if word not in list(stopwords_custom['stopwords'])]\n",
    "        lemmatized_documents_.append(lemmatized_document)\n",
    "        \n",
    "    # document-term matrix\n",
    "    vectorizer = CountVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
    "    dtm = vectorizer.fit_transform(lemmatized_documents_)\n",
    "    \n",
    "    term_frequency = pd.DataFrame(data={'word': vectorizer.get_feature_names(), \n",
    "                                      'frequency': dtm.sum(axis=0).flat\n",
    "                                      })\n",
    "    tf_50 = term_frequency.sort_values(by='frequency', ascending=False).head(50) # top 50 terms\n",
    "    tf_50.to_csv('preprocessing/term_frequency/tf_' + hotel)\n",
    "    \n",
    "    tf = term_frequency.sort_values(by='frequency', ascending=False)\n",
    "    column_list = list(tf['word'])\n",
    "    \n",
    "    df_tf = pd.DataFrame(data=dtm.todense(), index=data['review_id'], columns=vectorizer.get_feature_names())\n",
    "    df_tf = df_tf[column_list] # reordering columns\n",
    "    df_tf.to_csv('preprocessing/document_term_matrix/dt_' + hotel)\n",
    "    \n",
    "    # tf-idf matrix\n",
    "    transformer = TfidfTransformer()\n",
    "    tf_idf = transformer.fit_transform(dtm)\n",
    "    \n",
    "    df_tf_idf = pd.DataFrame(data=tf_idf.todense(), index=data['review_id'], columns=vectorizer.get_feature_names())\n",
    "    df_tf_idf = df_tf_idf[column_list] # reordering columns\n",
    "    df_tf_idf.to_csv('preprocessing/tf_idf_matrix/tf_idf_' + hotel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Matrix for non-partnership reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, hotel in enumerate(hotel_list):\n",
    "    print(str(i + 1) + 'th hotel out of ' + str(len(hotel_list)) + ' hotels.')\n",
    "\n",
    "    data = pd.read_csv('crawled_data/' + hotel, sep='|', usecols=['review_id', 'review', 'isPartner'])\n",
    "    data = data[data['isPartner'] == 0].reset_index(drop=True)\n",
    "    \n",
    "    # tokenizing\n",
    "    data['review'] = data['review'].apply(lambda review: re.sub(pattern='[^a-zA-Z]', repl=' ', string=review)) # remove non-English character\n",
    "    data['review'] = data['review'].apply(str.lower) # to lower-case\n",
    "    data['review'] = data['review'].apply(word_tokenize)\n",
    "    \n",
    "    # pos tagging\n",
    "    documents = list(data['review'])\n",
    "    pos_documents = [pos_tag(document) for document in documents]\n",
    "    \n",
    "    # stopwords\n",
    "    stop_documents = []\n",
    "      \n",
    "    for pos_document in pos_documents:\n",
    "        stop_document = [tagged_word for tagged_word in pos_document if tagged_word[0] not in stopwords.words('english')]\n",
    "        stop_document = [tagged_word for tagged_word in stop_document if tagged_word[0] not in list(stopwords_hotel['stopwords'])]\n",
    "        stop_document = [tagged_word for tagged_word in stop_document if tagged_word[1].startswith(('J', 'V', 'N', 'R'))]\n",
    "        stop_documents.append(stop_document)\n",
    "        \n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_documents = []\n",
    "    \n",
    "    for stop_document in stop_documents:\n",
    "        lemmatized_document = [lemmatizer.lemmatize(tagged_word[0], pos=get_wordnet_pos(tagged_word[1])) for tagged_word in stop_document]\n",
    "        lemmatized_documents.append(lemmatized_document)\n",
    "    \n",
    "    # stopwords_custom\n",
    "    lemmatized_documents_ = []\n",
    "    \n",
    "    for lemmatized_document in lemmatized_documents:\n",
    "        lemmatized_document = [word for word in lemmatized_document if word not in list(stopwords_custom['stopwords'])]\n",
    "        lemmatized_documents_.append(lemmatized_document)\n",
    "        \n",
    "    # document-term matrix\n",
    "    vectorizer = CountVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
    "    dtm = vectorizer.fit_transform(lemmatized_documents_)\n",
    "    \n",
    "    term_frequency = pd.DataFrame(data={'word': vectorizer.get_feature_names(), \n",
    "                                      'frequency': dtm.sum(axis=0).flat\n",
    "                                      })\n",
    "    tf_50 = term_frequency.sort_values(by='frequency', ascending=False).head(50) # top 50 terms\n",
    "    tf_50.to_csv('preprocessing/isPartner/term_frequency/tf_' + hotel)\n",
    "    \n",
    "    tf = term_frequency.sort_values(by='frequency', ascending=False)\n",
    "    column_list = list(tf['word'])\n",
    "    \n",
    "    df_tf = pd.DataFrame(data=dtm.todense(), index=data['review_id'], columns=vectorizer.get_feature_names())\n",
    "    df_tf = df_tf[column_list] # reordering columns\n",
    "    df_tf.to_csv('preprocessing/isPartner/document_term_matrix/dt_' + hotel)\n",
    "    \n",
    "    # tf-idf matrix\n",
    "    transformer = TfidfTransformer()\n",
    "    tf_idf = transformer.fit_transform(dtm)\n",
    "    \n",
    "    df_tf_idf = pd.DataFrame(data=tf_idf.todense(), index=data['review_id'], columns=vectorizer.get_feature_names())\n",
    "    df_tf_idf = df_tf_idf[column_list] # reordering columns\n",
    "    df_tf_idf.to_csv('preprocessing/isPartner/tf_idf_matrix/tf_idf_' + hotel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
